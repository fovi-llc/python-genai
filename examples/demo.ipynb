{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python GenAI Demo\n",
    "\n",
    "This notebook demonstrates the Python wrapper for Chrome's built-in AI Prompt API.\n",
    "\n",
    "**Requirements:**\n",
    "- Chrome browser with Prompt API enabled\n",
    "- Running in JupyterLab or Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: anywidget in /opt/conda/lib/python3.13/site-packages (0.9.21)\n",
      "Requirement already satisfied: ipywidgets in /opt/conda/lib/python3.13/site-packages (8.1.8)\n",
      "Requirement already satisfied: psygnal>=0.8.1 in /opt/conda/lib/python3.13/site-packages (from anywidget) (0.15.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.13/site-packages (from anywidget) (4.15.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (9.7.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/conda/lib/python3.13/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/conda/lib/python3.13/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/conda/lib/python3.13/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.13/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure_eval in /opt/conda/lib/python3.13/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install anywidget ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ab14d81d2fc4cf3913da197ec1f953e",
       "version_major": 2,
       "version_minor": 1
      },
      "text/plain": [
       "<__main__.LanguageModelWidget object at 0x7db714dbd7f0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anywidget\n",
    "import traitlets\n",
    "\n",
    "class LanguageModelWidget(anywidget.AnyWidget):\n",
    "    _esm = \"\"\"\n",
    "    function render({ model, el }) {\n",
    "      if (!('LanguageModel' in self)) {\n",
    "        model.set(\"result\", `Your browser doesn't support the Prompt API. If you're on Chrome, join the <a href=\"https://goo.gle/chrome-ai-dev-preview-join\">Early Preview Program</a> to enable it.`);\n",
    "      } else {\n",
    "        model.set(\"result\", \"calling\");\n",
    "        LanguageModel.availability()\n",
    "          .then(availability => { model.set(\"result\", availability); model.save_changes(); })\n",
    "          .catch(error => { model.set(\"result\", error); model.save_changes(); });\n",
    "      }\n",
    "      model.on(\"change:count\", () => {\n",
    "        model.set(\"result\", \"checking\");\n",
    "        console.log(\"checking\");\n",
    "        LanguageModel.availability()\n",
    "          .then(availability => { model.set(\"result\", availability); model.save_changes(); })\n",
    "          .catch(error => { model.set(\"result\", error); model.save_changes(); });\n",
    "      });\n",
    "    }\n",
    "\texport default { render };\n",
    "    \"\"\"\n",
    "    count = traitlets.Int(0).tag(sync=True)\n",
    "    prompt = traitlets.Unicode(\"Foo\").tag(sync=True)\n",
    "    result = traitlets.Unicode(\"Bar\").tag(sync=True)\n",
    "    \n",
    "\n",
    "counter = LanguageModelWidget()\n",
    "counter.count = 42\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unavailable'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.count = 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'unavailable'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the library\n",
    "from python_genai import LanguageModel, Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = await LanguageModel.create()\n",
    "lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Availability\n",
    "\n",
    "First, let's check if the Prompt API is available in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the API is available\n",
    "availability = await LanguageModel.availability()\n",
    "print(f\"Model availability: {availability}\")\n",
    "\n",
    "if availability == Availability.UNAVAILABLE:\n",
    "    print(\"\\n❌ Chrome Prompt API is not available.\")\n",
    "    print(\"Make sure you're running in Chrome with the API enabled.\")\n",
    "elif availability == Availability.DOWNLOADING:\n",
    "    print(\"\\n⏳ Model is downloading. Please wait and try again.\")\n",
    "elif availability == Availability.AVAILABLE:\n",
    "    print(\"\\n✅ API is ready to use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Model Parameters\n",
    "\n",
    "Let's see what parameters the model supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = await LanguageModel.params()\n",
    "if params:\n",
    "    print(f\"Default temperature: {params.default_temperature}\")\n",
    "    print(f\"Max temperature: {params.max_temperature}\")\n",
    "    print(f\"Default top-K: {params.default_top_k}\")\n",
    "    print(f\"Max top-K: {params.max_top_k}\")\n",
    "else:\n",
    "    print(\"Model parameters not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Prompt\n",
    "\n",
    "Create a session and send a simple prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session\n",
    "session = await LanguageModel.create()\n",
    "\n",
    "# Send a prompt\n",
    "result = await session.prompt(\"Write a haiku about Python programming.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming Response\n",
    "\n",
    "Stream the response as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Response: \", end=\"\")\n",
    "async for chunk in session.prompt_streaming(\"Tell me a short joke about computers.\"):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "Use a system prompt to set the context for the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a session with a system prompt\n",
    "assistant_session = await LanguageModel.create({\n",
    "    \"initialPrompts\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful Python programming assistant who gives concise answers.\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Ask a question\n",
    "result = await assistant_session.prompt(\"How do I read a CSV file in Python?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-turn Conversation\n",
    "\n",
    "Have a conversation with context maintained across prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First message\n",
    "result1 = await assistant_session.prompt(\"What is a list comprehension?\")\n",
    "print(\"Assistant:\", result1)\n",
    "print()\n",
    "\n",
    "# Follow-up message (the assistant remembers the context)\n",
    "result2 = await assistant_session.prompt(\"Can you show me an example?\")\n",
    "print(\"Assistant:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Token Usage\n",
    "\n",
    "Monitor how many tokens you've used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Current usage: {assistant_session.input_usage}/{assistant_session.input_quota} tokens\")\n",
    "\n",
    "# Measure how many tokens a prompt would use\n",
    "usage = await assistant_session.measure_input_usage(\"What is machine learning?\")\n",
    "print(f\"\\nThis prompt would use approximately {usage} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "Use JSON schema to get structured responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define a JSON schema\n",
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"required\": [\"sentiment\", \"score\"],\n",
    "    \"properties\": {\n",
    "        \"sentiment\": {\n",
    "            \"type\": \"string\",\n",
    "            \"enum\": [\"positive\", \"negative\", \"neutral\"]\n",
    "        },\n",
    "        \"score\": {\n",
    "            \"type\": \"number\",\n",
    "            \"minimum\": 0,\n",
    "            \"maximum\": 1\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Get structured response\n",
    "result = await session.prompt(\n",
    "    \"Analyze this review: The product exceeded all my expectations! Absolutely amazing!\",\n",
    "    {\"responseConstraint\": schema}\n",
    ")\n",
    "\n",
    "# Parse the JSON response\n",
    "data = json.loads(result)\n",
    "print(f\"Sentiment: {data['sentiment']}\")\n",
    "print(f\"Score: {data['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session Cloning\n",
    "\n",
    "Clone a session to create different conversation branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a story\n",
    "story_session = await LanguageModel.create({\n",
    "    \"initialPrompts\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a creative storyteller.\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "await story_session.prompt(\"Once upon a time, there was a dragon.\")\n",
    "\n",
    "# Create two different story branches\n",
    "branch1 = await story_session.clone()\n",
    "branch2 = await story_session.clone()\n",
    "\n",
    "result1 = await branch1.prompt(\"The dragon was friendly and helpful.\")\n",
    "print(\"Branch 1:\", result1)\n",
    "print()\n",
    "\n",
    "result2 = await branch2.prompt(\"The dragon was fierce and terrifying.\")\n",
    "print(\"Branch 2:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Destroy sessions when you're done to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await session.destroy()\n",
    "await assistant_session.destroy()\n",
    "await story_session.destroy()\n",
    "await branch1.destroy()\n",
    "await branch2.destroy()\n",
    "\n",
    "print(\"✅ All sessions destroyed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
